{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/OvertheSkyy/iskobot-rag-system/blob/main/groq.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4d1b897a",
      "metadata": {
        "id": "4d1b897a"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jerryjliu/llama_index/blob/main/docs/docs/examples/llm/groq.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2e33dced-e587-4397-81b3-d6606aa1738a",
      "metadata": {
        "id": "2e33dced-e587-4397-81b3-d6606aa1738a"
      },
      "source": [
        "# Groq\n",
        "\n",
        "Welcome to Groq! ğŸš€ At Groq, we've developed the world's first Language Processing Unitâ„¢, or LPU. The Groq LPU has a deterministic, single core streaming architecture that sets the standard for GenAI inference speed with predictable and repeatable performance for any given workload.\n",
        "\n",
        "Beyond the architecture, our software is designed to empower developers like you with the tools you need to create innovative, powerful AI applications. With Groq as your engine, you can:\n",
        "\n",
        "* Achieve uncompromised low latency and performance for real-time AI and HPC inferences ğŸ”¥\n",
        "* Know the exact performance and compute time for any given workload ğŸ”®\n",
        "* Take advantage of our cutting-edge technology to stay ahead of the competition ğŸ’ª\n",
        "\n",
        "Want more Groq? Check out our [website](https://groq.com) for more resources and join our [Discord community](https://discord.gg/JvNsBDKeCG) to connect with our developers!"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5863dde9-84a0-4c33-ad52-cc767442f63f",
      "metadata": {
        "id": "5863dde9-84a0-4c33-ad52-cc767442f63f"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "833bdb2b",
      "metadata": {
        "id": "833bdb2b"
      },
      "source": [
        "If you're opening this Notebook on colab, you will probably need to install LlamaIndex ğŸ¦™."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "4aff387e",
      "metadata": {
        "id": "4aff387e",
        "outputId": "d68eecce-dd8f-47a9-91ea-33bceb9fce52",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting llama-index-llms-groq\n",
            "  Downloading llama_index_llms_groq-0.2.0-py3-none-any.whl.metadata (2.3 kB)\n",
            "Collecting llama-index-core<0.12.0,>=0.11.0 (from llama-index-llms-groq)\n",
            "  Downloading llama_index_core-0.11.4-py3-none-any.whl.metadata (2.4 kB)\n",
            "Collecting llama-index-llms-openai-like<0.3.0,>=0.2.0 (from llama-index-llms-groq)\n",
            "  Downloading llama_index_llms_openai_like-0.2.0-py3-none-any.whl.metadata (753 bytes)\n",
            "Requirement already satisfied: PyYAML>=6.0.1 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-llms-groq) (6.0.2)\n",
            "Requirement already satisfied: SQLAlchemy>=1.4.49 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.12.0,>=0.11.0->llama-index-llms-groq) (2.0.32)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.6 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-llms-groq) (3.10.5)\n",
            "Collecting dataclasses-json (from llama-index-core<0.12.0,>=0.11.0->llama-index-llms-groq)\n",
            "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
            "Collecting deprecated>=1.2.9.3 (from llama-index-core<0.12.0,>=0.11.0->llama-index-llms-groq)\n",
            "  Downloading Deprecated-1.2.14-py2.py3-none-any.whl.metadata (5.4 kB)\n",
            "Collecting dirtyjson<2.0.0,>=1.0.8 (from llama-index-core<0.12.0,>=0.11.0->llama-index-llms-groq)\n",
            "  Downloading dirtyjson-1.0.8-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-llms-groq) (2024.6.1)\n",
            "Collecting httpx (from llama-index-core<0.12.0,>=0.11.0->llama-index-llms-groq)\n",
            "  Downloading httpx-0.27.2-py3-none-any.whl.metadata (7.1 kB)\n",
            "Requirement already satisfied: nest-asyncio<2.0.0,>=1.5.8 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-llms-groq) (1.6.0)\n",
            "Requirement already satisfied: networkx>=3.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-llms-groq) (3.3)\n",
            "Collecting nltk>3.8.1 (from llama-index-core<0.12.0,>=0.11.0->llama-index-llms-groq)\n",
            "  Downloading nltk-3.9.1-py3-none-any.whl.metadata (2.9 kB)\n",
            "Requirement already satisfied: numpy<2.0.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-llms-groq) (1.26.4)\n",
            "Requirement already satisfied: pillow>=9.0.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-llms-groq) (9.4.0)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-llms-groq) (2.8.2)\n",
            "Requirement already satisfied: requests>=2.31.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-llms-groq) (2.32.3)\n",
            "Collecting tenacity!=8.4.0,<9.0.0,>=8.2.0 (from llama-index-core<0.12.0,>=0.11.0->llama-index-llms-groq)\n",
            "  Downloading tenacity-8.5.0-py3-none-any.whl.metadata (1.2 kB)\n",
            "Collecting tiktoken>=0.3.3 (from llama-index-core<0.12.0,>=0.11.0->llama-index-llms-groq)\n",
            "  Downloading tiktoken-0.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.66.1 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-llms-groq) (4.66.5)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-llms-groq) (4.12.2)\n",
            "Collecting typing-inspect>=0.8.0 (from llama-index-core<0.12.0,>=0.11.0->llama-index-llms-groq)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-llms-groq) (1.16.0)\n",
            "Collecting llama-index-llms-openai<0.3.0,>=0.2.0 (from llama-index-llms-openai-like<0.3.0,>=0.2.0->llama-index-llms-groq)\n",
            "  Downloading llama_index_llms_openai-0.2.1-py3-none-any.whl.metadata (705 bytes)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.37.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-llms-openai-like<0.3.0,>=0.2.0->llama-index-llms-groq) (4.42.4)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.12.0,>=0.11.0->llama-index-llms-groq) (2.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.12.0,>=0.11.0->llama-index-llms-groq) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.12.0,>=0.11.0->llama-index-llms-groq) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.12.0,>=0.11.0->llama-index-llms-groq) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.12.0,>=0.11.0->llama-index-llms-groq) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.12.0,>=0.11.0->llama-index-llms-groq) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.12.0,>=0.11.0->llama-index-llms-groq) (4.0.3)\n",
            "Collecting llama-index-agent-openai<0.4.0,>=0.3.0 (from llama-index-llms-openai<0.3.0,>=0.2.0->llama-index-llms-openai-like<0.3.0,>=0.2.0->llama-index-llms-groq)\n",
            "  Downloading llama_index_agent_openai-0.3.0-py3-none-any.whl.metadata (728 bytes)\n",
            "Collecting openai<2.0.0,>=1.40.0 (from llama-index-llms-openai<0.3.0,>=0.2.0->llama-index-llms-openai-like<0.3.0,>=0.2.0->llama-index-llms-groq)\n",
            "  Downloading openai-1.43.0-py3-none-any.whl.metadata (22 kB)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk>3.8.1->llama-index-core<0.12.0,>=0.11.0->llama-index-llms-groq) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk>3.8.1->llama-index-core<0.12.0,>=0.11.0->llama-index-llms-groq) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk>3.8.1->llama-index-core<0.12.0,>=0.11.0->llama-index-llms-groq) (2024.5.15)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.7.0->llama-index-core<0.12.0,>=0.11.0->llama-index-llms-groq) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.20.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.7.0->llama-index-core<0.12.0,>=0.11.0->llama-index-llms-groq) (2.20.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.31.0->llama-index-core<0.12.0,>=0.11.0->llama-index-llms-groq) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.31.0->llama-index-core<0.12.0,>=0.11.0->llama-index-llms-groq) (3.8)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.31.0->llama-index-core<0.12.0,>=0.11.0->llama-index-llms-groq) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.31.0->llama-index-core<0.12.0,>=0.11.0->llama-index-llms-groq) (2024.7.4)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy>=1.4.49->SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.12.0,>=0.11.0->llama-index-llms-groq) (3.0.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.37.0->llama-index-llms-openai-like<0.3.0,>=0.2.0->llama-index-llms-groq) (3.15.4)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.37.0->llama-index-llms-openai-like<0.3.0,>=0.2.0->llama-index-llms-groq) (0.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.37.0->llama-index-llms-openai-like<0.3.0,>=0.2.0->llama-index-llms-groq) (24.1)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.37.0->llama-index-llms-openai-like<0.3.0,>=0.2.0->llama-index-llms-groq) (0.4.4)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.37.0->llama-index-llms-openai-like<0.3.0,>=0.2.0->llama-index-llms-groq) (0.19.1)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect>=0.8.0->llama-index-core<0.12.0,>=0.11.0->llama-index-llms-groq)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json->llama-index-core<0.12.0,>=0.11.0->llama-index-llms-groq)\n",
            "  Downloading marshmallow-3.22.0-py3-none-any.whl.metadata (7.2 kB)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx->llama-index-core<0.12.0,>=0.11.0->llama-index-llms-groq) (3.7.1)\n",
            "Collecting httpcore==1.* (from httpx->llama-index-core<0.12.0,>=0.11.0->llama-index-llms-groq)\n",
            "  Downloading httpcore-1.0.5-py3-none-any.whl.metadata (20 kB)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx->llama-index-core<0.12.0,>=0.11.0->llama-index-llms-groq) (1.3.1)\n",
            "Collecting h11<0.15,>=0.13 (from httpcore==1.*->httpx->llama-index-core<0.12.0,>=0.11.0->llama-index-llms-groq)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl.metadata (8.2 kB)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai<2.0.0,>=1.40.0->llama-index-llms-openai<0.3.0,>=0.2.0->llama-index-llms-openai-like<0.3.0,>=0.2.0->llama-index-llms-groq) (1.7.0)\n",
            "Collecting jiter<1,>=0.4.0 (from openai<2.0.0,>=1.40.0->llama-index-llms-openai<0.3.0,>=0.2.0->llama-index-llms-openai-like<0.3.0,>=0.2.0->llama-index-llms-groq)\n",
            "  Downloading jiter-0.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.6 kB)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx->llama-index-core<0.12.0,>=0.11.0->llama-index-llms-groq) (1.2.2)\n",
            "Downloading llama_index_llms_groq-0.2.0-py3-none-any.whl (2.9 kB)\n",
            "Downloading llama_index_core-0.11.4-py3-none-any.whl (1.6 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading llama_index_llms_openai_like-0.2.0-py3-none-any.whl (3.1 kB)\n",
            "Downloading Deprecated-1.2.14-py2.py3-none-any.whl (9.6 kB)\n",
            "Downloading dirtyjson-1.0.8-py3-none-any.whl (25 kB)\n",
            "Downloading llama_index_llms_openai-0.2.1-py3-none-any.whl (12 kB)\n",
            "Downloading nltk-3.9.1-py3-none-any.whl (1.5 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m53.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tenacity-8.5.0-py3-none-any.whl (28 kB)\n",
            "Downloading tiktoken-0.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m46.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Downloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
            "Downloading httpx-0.27.2-py3-none-any.whl (76 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m76.4/76.4 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httpcore-1.0.5-py3-none-any.whl (77 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading llama_index_agent_openai-0.3.0-py3-none-any.whl (13 kB)\n",
            "Downloading marshmallow-3.22.0-py3-none-any.whl (49 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m49.3/49.3 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Downloading openai-1.43.0-py3-none-any.whl (365 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m365.7/365.7 kB\u001b[0m \u001b[31m25.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jiter-0.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (318 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m318.9/318.9 kB\u001b[0m \u001b[31m21.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: dirtyjson, tenacity, nltk, mypy-extensions, marshmallow, jiter, h11, deprecated, typing-inspect, tiktoken, httpcore, httpx, dataclasses-json, openai, llama-index-core, llama-index-agent-openai, llama-index-llms-openai, llama-index-llms-openai-like, llama-index-llms-groq\n",
            "  Attempting uninstall: tenacity\n",
            "    Found existing installation: tenacity 9.0.0\n",
            "    Uninstalling tenacity-9.0.0:\n",
            "      Successfully uninstalled tenacity-9.0.0\n",
            "  Attempting uninstall: nltk\n",
            "    Found existing installation: nltk 3.8.1\n",
            "    Uninstalling nltk-3.8.1:\n",
            "      Successfully uninstalled nltk-3.8.1\n",
            "Successfully installed dataclasses-json-0.6.7 deprecated-1.2.14 dirtyjson-1.0.8 h11-0.14.0 httpcore-1.0.5 httpx-0.27.2 jiter-0.5.0 llama-index-agent-openai-0.3.0 llama-index-core-0.11.4 llama-index-llms-groq-0.2.0 llama-index-llms-openai-0.2.1 llama-index-llms-openai-like-0.2.0 marshmallow-3.22.0 mypy-extensions-1.0.0 nltk-3.9.1 openai-1.43.0 tenacity-8.5.0 tiktoken-0.7.0 typing-inspect-0.9.0\n"
          ]
        }
      ],
      "source": [
        "!pip install llama-index-llms-groq"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "9bbbc106",
      "metadata": {
        "id": "9bbbc106",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "86888c07-e788-4a79-ca2c-ccdde61bc1e8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting llama-index\n",
            "  Downloading llama_index-0.11.4-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: llama-index-agent-openai<0.4.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from llama-index) (0.3.0)\n",
            "Collecting llama-index-cli<0.4.0,>=0.3.0 (from llama-index)\n",
            "  Downloading llama_index_cli-0.3.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: llama-index-core<0.12.0,>=0.11.4 in /usr/local/lib/python3.10/dist-packages (from llama-index) (0.11.4)\n",
            "Collecting llama-index-embeddings-openai<0.3.0,>=0.2.0 (from llama-index)\n",
            "  Downloading llama_index_embeddings_openai-0.2.4-py3-none-any.whl.metadata (635 bytes)\n",
            "Collecting llama-index-indices-managed-llama-cloud>=0.3.0 (from llama-index)\n",
            "  Downloading llama_index_indices_managed_llama_cloud-0.3.0-py3-none-any.whl.metadata (3.8 kB)\n",
            "Collecting llama-index-legacy<0.10.0,>=0.9.48 (from llama-index)\n",
            "  Downloading llama_index_legacy-0.9.48.post3-py3-none-any.whl.metadata (8.5 kB)\n",
            "Requirement already satisfied: llama-index-llms-openai<0.3.0,>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from llama-index) (0.2.1)\n",
            "Collecting llama-index-multi-modal-llms-openai<0.3.0,>=0.2.0 (from llama-index)\n",
            "  Downloading llama_index_multi_modal_llms_openai-0.2.0-py3-none-any.whl.metadata (728 bytes)\n",
            "Collecting llama-index-program-openai<0.3.0,>=0.2.0 (from llama-index)\n",
            "  Downloading llama_index_program_openai-0.2.0-py3-none-any.whl.metadata (766 bytes)\n",
            "Collecting llama-index-question-gen-openai<0.3.0,>=0.2.0 (from llama-index)\n",
            "  Downloading llama_index_question_gen_openai-0.2.0-py3-none-any.whl.metadata (785 bytes)\n",
            "Collecting llama-index-readers-file<0.3.0,>=0.2.0 (from llama-index)\n",
            "  Downloading llama_index_readers_file-0.2.0-py3-none-any.whl.metadata (5.4 kB)\n",
            "Collecting llama-index-readers-llama-parse>=0.3.0 (from llama-index)\n",
            "  Downloading llama_index_readers_llama_parse-0.3.0-py3-none-any.whl.metadata (3.5 kB)\n",
            "Requirement already satisfied: nltk>3.8.1 in /usr/local/lib/python3.10/dist-packages (from llama-index) (3.9.1)\n",
            "Requirement already satisfied: openai>=1.14.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-agent-openai<0.4.0,>=0.3.0->llama-index) (1.43.0)\n",
            "Requirement already satisfied: PyYAML>=6.0.1 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.12.0,>=0.11.4->llama-index) (6.0.2)\n",
            "Requirement already satisfied: SQLAlchemy>=1.4.49 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.12.0,>=0.11.4->llama-index) (2.0.32)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.6 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.12.0,>=0.11.4->llama-index) (3.10.5)\n",
            "Requirement already satisfied: dataclasses-json in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.12.0,>=0.11.4->llama-index) (0.6.7)\n",
            "Requirement already satisfied: deprecated>=1.2.9.3 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.12.0,>=0.11.4->llama-index) (1.2.14)\n",
            "Requirement already satisfied: dirtyjson<2.0.0,>=1.0.8 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.12.0,>=0.11.4->llama-index) (1.0.8)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.12.0,>=0.11.4->llama-index) (2024.6.1)\n",
            "Requirement already satisfied: httpx in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.12.0,>=0.11.4->llama-index) (0.27.2)\n",
            "Requirement already satisfied: nest-asyncio<2.0.0,>=1.5.8 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.12.0,>=0.11.4->llama-index) (1.6.0)\n",
            "Requirement already satisfied: networkx>=3.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.12.0,>=0.11.4->llama-index) (3.3)\n",
            "Requirement already satisfied: numpy<2.0.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.12.0,>=0.11.4->llama-index) (1.26.4)\n",
            "Requirement already satisfied: pillow>=9.0.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.12.0,>=0.11.4->llama-index) (9.4.0)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.12.0,>=0.11.4->llama-index) (2.8.2)\n",
            "Requirement already satisfied: requests>=2.31.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.12.0,>=0.11.4->llama-index) (2.32.3)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.2.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.12.0,>=0.11.4->llama-index) (8.5.0)\n",
            "Requirement already satisfied: tiktoken>=0.3.3 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.12.0,>=0.11.4->llama-index) (0.7.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.66.1 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.12.0,>=0.11.4->llama-index) (4.66.5)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.12.0,>=0.11.4->llama-index) (4.12.2)\n",
            "Requirement already satisfied: typing-inspect>=0.8.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.12.0,>=0.11.4->llama-index) (0.9.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.12.0,>=0.11.4->llama-index) (1.16.0)\n",
            "Collecting llama-cloud>=0.0.11 (from llama-index-indices-managed-llama-cloud>=0.3.0->llama-index)\n",
            "  Downloading llama_cloud-0.0.15-py3-none-any.whl.metadata (751 bytes)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from llama-index-legacy<0.10.0,>=0.9.48->llama-index) (2.1.4)\n",
            "Requirement already satisfied: beautifulsoup4<5.0.0,>=4.12.3 in /usr/local/lib/python3.10/dist-packages (from llama-index-readers-file<0.3.0,>=0.2.0->llama-index) (4.12.3)\n",
            "Collecting pypdf<5.0.0,>=4.0.1 (from llama-index-readers-file<0.3.0,>=0.2.0->llama-index)\n",
            "  Downloading pypdf-4.3.1-py3-none-any.whl.metadata (7.4 kB)\n",
            "Collecting striprtf<0.0.27,>=0.0.26 (from llama-index-readers-file<0.3.0,>=0.2.0->llama-index)\n",
            "  Downloading striprtf-0.0.26-py3-none-any.whl.metadata (2.1 kB)\n",
            "Collecting llama-parse>=0.5.0 (from llama-index-readers-llama-parse>=0.3.0->llama-index)\n",
            "  Downloading llama_parse-0.5.1-py3-none-any.whl.metadata (4.5 kB)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk>3.8.1->llama-index) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk>3.8.1->llama-index) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk>3.8.1->llama-index) (2024.5.15)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.12.0,>=0.11.4->llama-index) (2.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.12.0,>=0.11.4->llama-index) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.12.0,>=0.11.4->llama-index) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.12.0,>=0.11.4->llama-index) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.12.0,>=0.11.4->llama-index) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.12.0,>=0.11.4->llama-index) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.12.0,>=0.11.4->llama-index) (4.0.3)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4<5.0.0,>=4.12.3->llama-index-readers-file<0.3.0,>=0.2.0->llama-index) (2.6)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx->llama-index-core<0.12.0,>=0.11.4->llama-index) (3.7.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx->llama-index-core<0.12.0,>=0.11.4->llama-index) (2024.7.4)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx->llama-index-core<0.12.0,>=0.11.4->llama-index) (1.0.5)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from httpx->llama-index-core<0.12.0,>=0.11.4->llama-index) (3.8)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx->llama-index-core<0.12.0,>=0.11.4->llama-index) (1.3.1)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx->llama-index-core<0.12.0,>=0.11.4->llama-index) (0.14.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai>=1.14.0->llama-index-agent-openai<0.4.0,>=0.3.0->llama-index) (1.7.0)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from openai>=1.14.0->llama-index-agent-openai<0.4.0,>=0.3.0->llama-index) (0.5.0)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.7.0->llama-index-core<0.12.0,>=0.11.4->llama-index) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.20.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.7.0->llama-index-core<0.12.0,>=0.11.4->llama-index) (2.20.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.31.0->llama-index-core<0.12.0,>=0.11.4->llama-index) (3.3.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.31.0->llama-index-core<0.12.0,>=0.11.4->llama-index) (2.0.7)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy>=1.4.49->SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.12.0,>=0.11.4->llama-index) (3.0.3)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from typing-inspect>=0.8.0->llama-index-core<0.12.0,>=0.11.4->llama-index) (1.0.0)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json->llama-index-core<0.12.0,>=0.11.4->llama-index) (3.22.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->llama-index-legacy<0.10.0,>=0.9.48->llama-index) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->llama-index-legacy<0.10.0,>=0.9.48->llama-index) (2024.1)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->llama-index-legacy<0.10.0,>=0.9.48->llama-index) (2024.1)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx->llama-index-core<0.12.0,>=0.11.4->llama-index) (1.2.2)\n",
            "Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.10/dist-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json->llama-index-core<0.12.0,>=0.11.4->llama-index) (24.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->llama-index-legacy<0.10.0,>=0.9.48->llama-index) (1.16.0)\n",
            "Downloading llama_index-0.11.4-py3-none-any.whl (6.8 kB)\n",
            "Downloading llama_index_cli-0.3.0-py3-none-any.whl (27 kB)\n",
            "Downloading llama_index_embeddings_openai-0.2.4-py3-none-any.whl (6.1 kB)\n",
            "Downloading llama_index_indices_managed_llama_cloud-0.3.0-py3-none-any.whl (9.5 kB)\n",
            "Downloading llama_index_legacy-0.9.48.post3-py3-none-any.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading llama_index_multi_modal_llms_openai-0.2.0-py3-none-any.whl (5.9 kB)\n",
            "Downloading llama_index_program_openai-0.2.0-py3-none-any.whl (5.3 kB)\n",
            "Downloading llama_index_question_gen_openai-0.2.0-py3-none-any.whl (2.9 kB)\n",
            "Downloading llama_index_readers_file-0.2.0-py3-none-any.whl (38 kB)\n",
            "Downloading llama_index_readers_llama_parse-0.3.0-py3-none-any.whl (2.5 kB)\n",
            "Downloading llama_cloud-0.0.15-py3-none-any.whl (180 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m180.2/180.2 kB\u001b[0m \u001b[31m14.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading llama_parse-0.5.1-py3-none-any.whl (9.5 kB)\n",
            "Downloading pypdf-4.3.1-py3-none-any.whl (295 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m295.8/295.8 kB\u001b[0m \u001b[31m21.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading striprtf-0.0.26-py3-none-any.whl (6.9 kB)\n",
            "Installing collected packages: striprtf, pypdf, llama-cloud, llama-parse, llama-index-readers-file, llama-index-legacy, llama-index-indices-managed-llama-cloud, llama-index-embeddings-openai, llama-index-readers-llama-parse, llama-index-program-openai, llama-index-question-gen-openai, llama-index-multi-modal-llms-openai, llama-index-cli, llama-index\n",
            "Successfully installed llama-cloud-0.0.15 llama-index-0.11.4 llama-index-cli-0.3.0 llama-index-embeddings-openai-0.2.4 llama-index-indices-managed-llama-cloud-0.3.0 llama-index-legacy-0.9.48.post3 llama-index-multi-modal-llms-openai-0.2.0 llama-index-program-openai-0.2.0 llama-index-question-gen-openai-0.2.0 llama-index-readers-file-0.2.0 llama-index-readers-llama-parse-0.3.0 llama-parse-0.5.1 pypdf-4.3.1 striprtf-0.0.26\n"
          ]
        }
      ],
      "source": [
        "!pip install llama-index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "ad297f19-998f-4485-aa2f-d67020058b7d",
      "metadata": {
        "id": "ad297f19-998f-4485-aa2f-d67020058b7d"
      },
      "outputs": [],
      "source": [
        "from llama_index.llms.groq import Groq"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata"
      ],
      "metadata": {
        "id": "VuncWWrBOZXR"
      },
      "id": "VuncWWrBOZXR",
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "152ced37-9a42-47be-9a39-4218521f5e72",
      "metadata": {
        "id": "152ced37-9a42-47be-9a39-4218521f5e72",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "147f25ca-3a37-48a3-c928-4571c2157022"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'llama-3.1-70b-versatile'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "llm = Groq(model=\"llama-3.1-70b-versatile\", api_key=userdata.get('GROQ_API_KEY') ) #Get Groq API here https://console.groq.com/keys then specify in secrets\n",
        "llm.model"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "562455fe",
      "metadata": {
        "id": "562455fe"
      },
      "source": [
        "A list of available LLM models can be found [here](https://console.groq.com/docs/models)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "d61b10bb-e911-47fb-8e84-19828cf224be",
      "metadata": {
        "id": "d61b10bb-e911-47fb-8e84-19828cf224be"
      },
      "outputs": [],
      "source": [
        "response = llm.complete(\"What is Retrieval Augmented Generation?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "3bd14f4e-c245-4384-a471-97e4ddfcb40e",
      "metadata": {
        "id": "3bd14f4e-c245-4384-a471-97e4ddfcb40e",
        "outputId": "00ec8ca7-5f4c-4001-a8dd-60b8ea9488bc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Retrieval Augmented Generation (RAG) is a type of natural language processing (NLP) model that combines the strengths of retrieval-based and generation-based approaches to produce more accurate and informative text. The main idea behind RAG is to use a retrieval system to fetch relevant information from a large database or knowledge base, and then use a generation model to produce text based on the retrieved information.\n",
            "\n",
            "In traditional generation-based models, the model is trained to generate text from scratch based on a given prompt or input. However, these models often struggle to produce accurate and informative text, especially when dealing with complex or domain-specific topics.\n",
            "\n",
            "Retrieval-based models, on the other hand, rely on a large database or knowledge base to retrieve relevant information and present it to the user. However, these models often lack the ability to generate coherent and fluent text.\n",
            "\n",
            "RAG models address these limitations by combining the strengths of both approaches. Here's how it works:\n",
            "\n",
            "1. **Retrieval**: The model uses a retrieval system to fetch relevant information from a large database or knowledge base based on the input prompt or query.\n",
            "2. **Ranking**: The retrieved information is ranked based on relevance, accuracy, and other factors to select the most relevant information.\n",
            "3. **Generation**: The selected information is then used as input to a generation model, which produces text based on the retrieved information.\n",
            "\n",
            "RAG models have several advantages over traditional generation-based models, including:\n",
            "\n",
            "* **Improved accuracy**: By retrieving relevant information from a large database or knowledge base, RAG models can produce more accurate and informative text.\n",
            "* **Increased coherence**: The generation model can produce coherent and fluent text based on the retrieved information.\n",
            "* **Better handling of complex topics**: RAG models can handle complex or domain-specific topics more effectively by retrieving relevant information from a large database or knowledge base.\n",
            "\n",
            "RAG models have been applied to various NLP tasks, including question answering, text summarization, and dialogue generation. They have shown promising results in these areas, and are considered a promising direction for future research in NLP.\n"
          ]
        }
      ],
      "source": [
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3ba9503c-b440-43c6-a50c-676c79993813",
      "metadata": {
        "id": "3ba9503c-b440-43c6-a50c-676c79993813"
      },
      "source": [
        "#### Call `chat` with a list of messages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "ee8a4a55-5680-4dc6-a44c-fc8ad7892f80",
      "metadata": {
        "id": "ee8a4a55-5680-4dc6-a44c-fc8ad7892f80"
      },
      "outputs": [],
      "source": [
        "from llama_index.core.llms import ChatMessage\n",
        "\n",
        "messages = [\n",
        "    ChatMessage(\n",
        "        role=\"system\", content=\"You are a university assistant chatbot named Iskobot.\"\n",
        "    ),\n",
        "    ChatMessage(role=\"user\", content=\"What is your name?\"),\n",
        "]\n",
        "resp = llm.chat(messages)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "2a9bfe53-d15b-4e75-9d91-8c5d024f4eda",
      "metadata": {
        "id": "2a9bfe53-d15b-4e75-9d91-8c5d024f4eda",
        "outputId": "e1ea6ad1-b47c-4d60-aa2c-e4bd2ec24e29",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "assistant: Hello, I'm Iskobot, your university assistant chatbot. I'm here to help you with any questions or information you need about the university. How can I assist you today?\n"
          ]
        }
      ],
      "source": [
        "print(resp)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "25ad1b00-28fc-4bcd-96c4-d5b35605721a",
      "metadata": {
        "id": "25ad1b00-28fc-4bcd-96c4-d5b35605721a"
      },
      "source": [
        "### Streaming"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "13c641fa-345a-4dce-87c5-ab1f6dcf4757",
      "metadata": {
        "id": "13c641fa-345a-4dce-87c5-ab1f6dcf4757"
      },
      "source": [
        "Using `stream_complete` endpoint"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "06da1ef1-2f6b-497c-847b-62dd2df11491",
      "metadata": {
        "id": "06da1ef1-2f6b-497c-847b-62dd2df11491"
      },
      "outputs": [],
      "source": [
        "response = llm.stream_complete(\"What is Retrieval Augmented Generation?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "1b851def-5160-46e5-a30c-5a3ef2356b79",
      "metadata": {
        "id": "1b851def-5160-46e5-a30c-5a3ef2356b79",
        "outputId": "c076f1bb-ab37-47d7-f554-14a14e466a63",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Retrieval Augmented Generation (RAG) is a type of natural language processing (NLP) model that combines the strengths of both retrieval-based and generation-based approaches to produce more accurate and informative text outputs.\n",
            "\n",
            "In traditional generation-based models, the model generates text from scratch based on the input prompt or context. However, these models can struggle to produce accurate and up-to-date information, especially when dealing with rare or domain-specific topics.\n",
            "\n",
            "Retrieval-based models, on the other hand, rely on a large database or knowledge graph to retrieve relevant information and provide answers to user queries. However, these models can struggle to generate coherent and fluent text, especially when the retrieved information is fragmented or incomplete.\n",
            "\n",
            "RAG models address these limitations by combining the strengths of both approaches. Here's how it works:\n",
            "\n",
            "1. **Retrieval**: The model first retrieves a set of relevant documents or passages from a large database or knowledge graph based on the input prompt or context.\n",
            "2. **Ranking**: The model then ranks the retrieved documents or passages based on their relevance to the input prompt or context.\n",
            "3. **Generation**: The model uses the top-ranked documents or passages as input to a generation-based model, which generates text based on the retrieved information.\n",
            "\n",
            "The key benefits of RAG models are:\n",
            "\n",
            "1. **Improved accuracy**: By retrieving relevant information from a large database or knowledge graph, RAG models can produce more accurate and up-to-date information.\n",
            "2. **Increased coherence**: By using the retrieved information as input to a generation-based model, RAG models can generate more coherent and fluent text.\n",
            "3. **Better handling of rare or domain-specific topics**: RAG models can handle rare or domain-specific topics more effectively by retrieving relevant information from a large database or knowledge graph.\n",
            "\n",
            "RAG models have been applied to a range of NLP tasks, including question answering, text summarization, and dialogue generation. They have shown promising results in improving the accuracy and coherence of text outputs, especially in domains where accurate and up-to-date information is critical."
          ]
        }
      ],
      "source": [
        "for r in response:\n",
        "    print(r.delta, end=\"\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ca52051d-6b28-49d7-98f5-82e266a1c7a6",
      "metadata": {
        "id": "ca52051d-6b28-49d7-98f5-82e266a1c7a6"
      },
      "source": [
        "Using `stream_chat` endpoint"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "fe553190-52a9-436d-84ae-4dd99a1808f4",
      "metadata": {
        "id": "fe553190-52a9-436d-84ae-4dd99a1808f4"
      },
      "outputs": [],
      "source": [
        "from llama_index.core.llms import ChatMessage\n",
        "\n",
        "messages = [\n",
        "    ChatMessage(\n",
        "        role=\"system\", content=\"You are a university assistant chatbot named Iskobot.\"\n",
        "    ),\n",
        "    ChatMessage(role=\"user\", content=\"What is your name?\"),\n",
        "]\n",
        "resp = llm.stream_chat(messages)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "154c503c-f893-4b6b-8a65-a9a27b636046",
      "metadata": {
        "id": "154c503c-f893-4b6b-8a65-a9a27b636046",
        "outputId": "5e2e9072-4713-479d-c354-d19a6730ac72",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello, I'm Iskobot, your university assistant chatbot. I'm here to help answer your questions and provide information about university life, courses, and more. How can I assist you today?"
          ]
        }
      ],
      "source": [
        "for r in resp:\n",
        "    print(r.delta, end=\"\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}